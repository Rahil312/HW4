# README for MPI Latency Analysis

## Overview
This experiment measures the average round-trip time (RTT) for MPI point-to-point communications across four scenarios. Each scenario tests a different number of node pairs communicating in parallel, and message sizes range from 32 KB to 2 MB. The RTT is calculated as the average of 10 iterations for each configuration, and the standard deviation is reported to highlight variability.

---

## Scenarios
1. **Scenario 1**: One pair of nodes communicating.
2. **Scenario 2**: Two pairs of nodes communicating in parallel.
3. **Scenario 3**: Three pairs of nodes communicating in parallel.
4. **Scenario 4**: Four pairs of nodes communicating in parallel.

Message sizes tested: **32 KB, 64 KB, 128 KB, 256 KB, 512 KB, 1 MB, and 2 MB**.

---

## Observations

### 1. Latency Trends Across Scenarios
- **Scenario 1**:
  - Exhibits the highest variability in latency, especially for larger message sizes such as 1 MB and 2 MB.
  - The average RTT for smaller messages (e.g., 32 KB and 64 KB) is relatively stable but higher compared to the other scenarios, likely due to initialization overhead.

- **Scenario 2**:
  - RTT decreases significantly compared to Scenario 1, showing better utilization of network resources with parallel communication.
  - Variability (as indicated by standard deviation) is low across all message sizes, indicating consistent performance.

- **Scenario 3**:
  - RTT is slightly higher than Scenario 2 for larger message sizes (1 MB and 2 MB), as the network starts experiencing more contention with three pairs of nodes communicating in parallel.

- **Scenario 4**:
  - Demonstrates the highest overall RTT for large message sizes (e.g., 2 MB), where average latency exceeds 350 microseconds.
  - The standard deviation is minimal across smaller message sizes but increases for larger messages, suggesting network saturation or contention under high load.

---

### 2. Message Size and Latency
- **General Trends**:
  - RTT increases with message size in all scenarios. Larger messages take longer to transmit, as they require more bandwidth and may experience delays due to packet fragmentation and reassembly.
  - The scalability of the network diminishes as the message size grows and the number of communicating pairs increases.

- **Unusual Data Points**:
  - In Scenario 1, the RTT for the 32 KB message size shows higher variability, with a standard deviation of 58.46 microseconds. This may indicate initialization overhead or interference during the measurements.
  - For all scenarios, RTT variability becomes more pronounced at message sizes of 1 MB and above, reflecting the impact of network contention under heavy load.

---

## Network Configuration Insights
1. **Scalability**:
   - The network scales well for smaller message sizes (up to 256 KB), with minimal increase in RTT across scenarios.
   - Scalability decreases for larger message sizes (512 KB and above), as RTT increases significantly when more pairs communicate simultaneously.

2. **Bandwidth and Saturation**:
   - The consistent increase in RTT for larger messages suggests that the network bandwidth is a limiting factor. Network saturation becomes evident in Scenario 4, where four pairs of nodes communicate simultaneously.

3. **Latency Variability**:
   - Standard deviation remains low for smaller messages in all scenarios but increases for larger messages, particularly in Scenarios 3 and 4. This highlights the impact of network contention as the load increases.

---

## Key Takeaways
1. **Performance**:
   - Smaller message sizes (32 KB to 256 KB) show low latency and high scalability, making them suitable for distributed systems requiring frequent communications.
   - Larger message sizes (512 KB and above) introduce significant latency, especially in high-load scenarios with multiple pairs communicating.

2. **Optimization Suggestions**:
   - Reducing message size, batching smaller messages, or implementing flow control could help mitigate latency for large-scale applications.
   - Upgrading the network infrastructure (e.g., increasing bandwidth or using dedicated communication links) could improve scalability and reduce RTT for larger messages.

3. **Future Improvements**:
   - Additional experiments on a dedicated high-speed network would help isolate the effects of network contention.
   - Investigating the root cause of outliers and analyzing network traffic during the experiment could provide further insights.

---

## Conclusion
This experiment provides valuable insights into the impact of message size and parallel communication on MPI performance. The results demonstrate the trade-offs between scalability, latency, and network contention in distributed systems, offering guidance for optimizing MPI-based applications.

